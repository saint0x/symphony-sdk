# Symphony SDK: Configuration & Syntax Reference

This document provides a detailed breakdown of the configuration syntax for core Symphony SDK entities (Tools, Agents, Teams, Pipelines) and describes key data structures used in the SDK. It complements `API.md` (for method signatures) and `USAGE.md` (for functional examples). For authoritative type definitions, always refer to the `src/types/sdk.ts` and `src/llm/types.ts` files.

## 1. Core Configuration Objects

### 1.1. `ToolConfig`

Defines a tool that can be registered with the `ToolRegistry` and used by agents or pipelines.

```typescript
// From: src/types/sdk.ts (simplified for brevity, see source for full details)
export interface ToolConfig {
  name: string;                       // Unique name of the tool.
  description?: string;                // Description of what the tool does (used in prompts).
  type: string;                       // Arbitrary type for categorization (e.g., 'file_system', 'communication').
  nlp?: string;                        // Optional: Natural language phrase for when this tool might be used.
  apiKey?: string;                     // Optional: API key if the tool calls an external service (prefer environment variables).
  timeout?: number;                    // Optional: Timeout for the tool's handler execution (milliseconds).
  retryCount?: number;                 // Optional: Number of retries for the handler (executor dependent).
  maxSize?: number;                    // Optional: e.g., max file size for a file tool.
  
  config: {                          // Tool-specific configurations.
    inputSchema?: Record<string, any>; // JSON Schema object defining the tool's input parameters.
                                     // This is crucial for structured LLM interaction.
    // ... other custom tool configurations ...
  };
  
  inputs?: string[];                   // Optional: Simple list of input parameter names (less descriptive than inputSchema).
  outputs?: string[];                  // Optional: Simple list of output field names from the result.
  capabilities?: string[];             // Optional: Tags describing tool capabilities.
  
  // Asynchronous function that executes the tool's logic.
  handler?: (params: any) => Promise<ToolResult<any>>;
}

export interface ToolResult<T = any> {
  success: boolean;
  result?: T;                         // The data returned by the tool on success.
  error?: string;                     // Error message on failure.
  metrics?: {                         // Optional: Execution metrics.
    duration: number;
    startTime: number;
    endTime: number;
  };
}
```
- **Key Fields**: `name`, `description`, `config.inputSchema`, `handler`.
- `config.inputSchema`: Crucial for agents to understand how to call the tool. Use JSON Schema format.
- `handler`: Must return a `Promise<ToolResult>`.

### 1.2. `LLMBaseConfig` (for Agent/Team LLM settings)

Defines LLM settings, used within `AgentConfig` and for agents in `TeamConfig`.

```typescript
// From: src/types/sdk.ts (previously had useFunctionCalling, now removed)
export interface LLMBaseConfig {
  model: string;         // LLM model identifier (e.g., 'gpt-3.5-turbo', 'gpt-4o-mini').
  provider?: string;      // Optional: LLM provider name (e.g., 'openai', 'anthropic'). 
                         // Defaults to global default if not specified.
  apiKey?: string;        // Optional: Provider-specific API key. Best practice is to set via environment variables.
  temperature?: number;   // Optional: Sampling temperature (e.g., 0.0 to 2.0). Default usually 0.7-1.0.
  maxTokens?: number;     // Optional: Maximum number of tokens to generate in the completion.
}
```
- An agent's `llm` property in `AgentConfig` can be this object or just a string (model name).

### 1.3. `AgentConfig`

Defines the configuration for an agent.

```typescript
// From: src/types/sdk.ts (simplified, see source for all fields like log, streamOptions etc.)
export interface AgentConfig {
  name: string;                        // Unique name for the agent.
  description: string;                 // Detailed description of the agent's purpose, expertise, and how it operates.
  task: string;                        // The primary or default task/goal the agent is designed to accomplish.
  tools: string[];                     // Array of tool names (must be registered in `ToolRegistry`) that this agent can use.
  llm: LLMBaseConfig | string;         // LLM configuration (object or model name string).
  
  directives?: string;                // Optional: Additional general instructions appended to the system prompt.
  systemPrompt?: string;              // Optional: Custom system prompt. If not provided, one is generated by `SystemPromptService`.
                                     // If tools are present, SDK always appends JSON structural guidance.
  maxCalls?: number;                  // Optional: Maximum LLM calls allowed for one `agent.run()` or `agentExecutor.executeTask()`.
  requireApproval?: boolean;          // Optional: Conceptual flag, if agent needs approval for certain steps (executor dependent).
  timeout?: number;                   // Optional: Overall timeout for agent task execution (milliseconds).
  capabilities?: string[];              // Optional: List of tags describing agent capabilities.
  enableCache?: boolean;              // Optional: Whether to use caching for this agent's LLM calls.
  // enableStreaming?: boolean;       // Optional, for streaming agent output.
  // streamOptions?: { ... };
  // log?: { ... };                   // Optional: Fine-grained logging controls for this agent.
}
```
- If `tools` array is non-empty, JSON mode is automatically activated by the SDK for LLM communication.

### 1.4. `TeamConfig`

Defines a team of collaborating agents.

```typescript
// From: src/types/sdk.ts (simplified)
export interface TeamConfig {
  name: string;
  description: string;
  agents: Array<string | AgentConfig>; // Member agents: by name (if registered) or full AgentConfig.
  capabilities?: string[];
  manager?: boolean | string;        // Optional: Designate a manager (true, or agent name/ID).
  strategy?: TeamStrategy;           // Optional: Coordination strategy for the team.
  delegationStrategy?: DelegationStrategy; // Optional: How tasks are delegated within the team.
  // log?: { ... };
}

export interface TeamStrategy {      // Conceptual structure from USAGE.md
  name?: string;
  description?: string;
  assignmentLogic?: (task: string, agents: AgentConfig[]) => Promise<string[]>; // Or agent names
  coordinationRules?: {
    maxParallelTasks?: number;
    taskTimeout?: number;
    // ... other rules
  };
}

export interface DelegationStrategy { // Conceptual structure from USAGE.md
  type: 'custom' | 'rule-based' | 'llm_driven';
  customLogic?: (task: string, agents: AgentConfig[]) => Promise<string[]>;
  rules?: Array<{ condition: string; assignTo: string[]; }>;
}
```
- `agents`: Can contain full `AgentConfig` objects for inline definition or strings referencing globally registered agents (if supported by `symphony.team.create`).

### 1.5. `PipelineConfig` and `PipelineStep`

Defines a multi-step workflow.

```typescript
// From: src/types/sdk.ts (simplified)
export interface PipelineConfig {
  name: string;
  description?: string;
  steps: PipelineStep[];
  variables?: Record<string, any>;     // Global variables for the pipeline, with defaults.
  errorStrategy?: {
    type: 'stop' | 'continue' | 'retry';
    maxAttempts?: number; // For pipeline-level retries
    delay?: number;
  };
  // metrics?: { ... };
}

export interface PipelineStep {
  id: string;                         // Unique ID for the step within the pipeline.
  name: string;                       // Human-readable name for the step.
  type: 'tool' | 'agent' | 'team';    // Type of operation this step performs.
                                      // Other types like 'condition', 'chain' might be conceptual or require custom tool implementation.
  tool?: string | ToolConfig;         // Tool name (if type='tool') or full ToolConfig.
  agent?: string;                     // Agent name/ID (if type='agent').
  team?: string;                      // Team name/ID (if type='team').
  
  inputs?: Record<string, string>;    // Maps step inputs from pipeline variables ('$var') or other step outputs ('@stepId.path.to.value').
  outputs?: Record<string, string>;   // Maps step outputs to pipeline context variables (e.g., { myOutputVar: '.result.someField' }).
  dependencies?: string[];            // Array of step IDs that must complete before this step runs.
  
  config?: Record<string, any>;       // Step-specific configuration (e.g., for a custom condition tool).
  retryConfig?: RetryConfig;          // Step-level retry configuration (see RetryConfig type).
  timeout?: number;                   // Step-level timeout in milliseconds.
  // chained?: number;                // For ToolChain steps, not directly part of generic PipelineStep.
  // expects?: Record<string, string>;
  // conditions?: { ... };
  // inputMap?: ... ;
  // handler?: ... ; // Handler is for ToolConfig, not directly on PipelineStep unless it's a fully defined tool step.
}

export interface RetryConfig { // From src/types/sdk.ts
    enabled: boolean;
    maxAttempts?: number;
    delay?: number;         // Initial delay in ms
    backoffFactor?: number;
    retryableErrors?: string[]; // Specific error messages or types to retry on
}
```
- `inputs` and `outputs` use a specific syntax: `$` for pipeline variables, `@` for step outputs.
- Agents and Teams used as steps benefit from their respective configurations, including JSON mode for tool-enabled agents.

## 2. Core LLM Interaction Data Structures

These types are defined in `src/llm/types.ts` and are fundamental to how the SDK interacts with LLMs.

### 2.1. `LLMMessage`

Represents a single message in a conversation with an LLM.

```typescript
export interface LLMMessage {
  role: 'system' | 'user' | 'assistant' | 'function' | 'tool';
  content: string | null;
  name?: string;        // For role: 'function' (legacy) or 'tool' (name of the function/tool that was called).
  tool_call_id?: string; // For role: 'tool' (correlates with a tool_calls.id from assistant message).
  tool_calls?: {        // For role: 'assistant' when it decides to call tools.
    id: string;         // ID for this specific tool call.
    type: 'function';   // Currently, only 'function' type is standard for OpenAI tool calls.
    function: {
      name: string;     // Name of the tool/function to be called.
      arguments: string;  // JSON string of arguments for the function.
    };
  }[];
}
```

### 2.2. `LLMRequest`

Represents a request to an LLM provider's `complete` or `completeStream` method.

```typescript
export interface LLMRequest {
  messages: LLMMessage[];
  expectsJsonResponse?: boolean; // SDK INTERNAL: Hint to provider that a JSON response is expected (e.g., for OpenAI to set response_format).
  
  // --- Fields primarily for OpenAI or OpenAI-compatible APIs ---
  functions?: LLMFunctionDefinition[]; // For legacy OpenAI function calling.
  functionCall?: { name: string; } | "auto" | "none"; // Legacy OpenAI function calling.
  tool_choice?: "none" | "auto" | { type: "function"; function: { name: string } }; // For OpenAI tool usage.
  response_format?: { type: "text" | "json_object" }; // For OpenAI, to request specific response formats.
  // --- End OpenAI specific fields ---
  
  temperature?: number;             // Overrides default temperature for this request.
  maxTokens?: number;               // Overrides default maxTokens for this request.
  stream?: boolean;                 // Whether to stream the response.
  provider?: string;                // Optional: specific provider to use for this request.
  llmConfig?: LLMRequestConfig;     // Optional: Request-specific overrides for LLM parameters (model, temp, etc. without API key).
}

export interface LLMFunctionDefinition { // Used for OpenAI functions/tools
    name: string;
    description: string;
    parameters: { type: 'object'; properties: Record<string, any>; required?: string[]; };
}
```
- `messages`: The core conversation history.
- `expectsJsonResponse`: Set by `AgentExecutor` if tools are present, signaling to providers (especially `OpenAIProvider`) to optimize for JSON output.
- OpenAI-specific fields (`functions`, `functionCall`, `tool_choice`, `response_format`) allow fine-grained control when using OpenAI models directly or through a provider that maps to these features.

### 2.3. `LLMResponse`

Represents the response from an LLM provider.

```typescript
export interface LLMResponse {
  content: string | null;             // The textual content of the LLM's response.
                                      // If JSON mode was successful, this will be a JSON string.
  model: string;                      // Model identifier that generated the response.
  role: 'assistant';
  // For OpenAI function/tool calls
  functionCall?: { id?: string; name: string; arguments: string; }; // Legacy
  tool_calls?: { id: string; type: 'function'; function: { name: string; arguments: string; }; }[];
  
  usage: {                          // Token usage information.
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
  metrics?: any; // Could include ExecutionMetrics from sdk.ts, plus specific tokenUsage
  toString(): string; // Utility to get string content.
}
```
- `content`: This is key. For tool-enabled agents, this is expected to be a JSON string parsable by `AgentExecutor`.
- `tool_calls`: If the LLM (like OpenAI) uses its native tool calling mechanism, this array will be populated.

## 3. Agent Execution Output

### 3.1. `AgentResult`

Standardized result object returned by `agent.run()` or `agentExecutor.executeTask()`.

```typescript
// From: src/types/sdk.ts (simplified)
export interface AgentResult<T = any> {
  success: boolean;
  result?: {                        // Main result data from the agent's execution.
    response: string;             // The final textual response or summary from the agent.
                                  // If a tool was called, this might be a summary of the tool execution.
                                  // If no tool was called (tool_name:"none"), this is the LLM's direct answer.
    reasoning?: string;            // Optional: Explanation or thought process from the agent.
    toolsExecuted?: ToolResult<any>[]; // Array of results from tools that were executed.
    // ... other agent-specific result fields ...
  };
  error?: string;                     // Error message if success is false.
  metrics?: {                         // Performance and usage metrics.
    duration: number;               // Total execution time in ms.
    startTime: number;
    endTime: number;
    toolCalls?: number;             // Number of tools invoked.
    llmUsage?: {                  // Token usage from LLM calls.
        promptTokens: number;
        completionTokens: number;
        totalTokens: number;
        model: string;
    };
    // ... other metrics ...
  };
}
```
- `success`: Indicates overall success of the task.
- `result.response`: The primary output to be presented to the user or consumed by another system.
- `result.toolsExecuted`: Provides details if tools were part of the execution.

---
This document outlines the key syntactical elements. Refer to `USAGE.md` for runnable examples and `src/types/` for all type details. 